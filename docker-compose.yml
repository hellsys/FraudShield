services:
  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks: [fraudnet]
    ports:
      - "5432:5432"

  rabbitmq:
    image: rabbitmq:3.13-management
    networks: [fraudnet]
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - "5673:5672"
      - "15673:15672"

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_PASS}
    volumes:
      - minio:/data
    ports:
      - "9000:9000"
      - "9001:9001"
    networks: [fraudnet]

    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 12            

  minio-mc:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy      
    volumes:
      - ./bootstrap:/bootstrap:ro       
    entrypoint: >
      /bin/sh -c '
        set -e
        echo "➜ configuring MinIO …"
        mc alias set myminio http://minio:9000 "$${MINIO_USER}" "$${MINIO_PASS}"
        mc mb -p myminio/"$${S3_BUCKET}" || true
        if [ -d /bootstrap ] && [ "$(ls -A /bootstrap)" ]; then
          echo "➜ uploading bootstrap files"
          mc cp --recursive /bootstrap/* myminio/"$${S3_BUCKET}"/
        fi
        echo "✓ MinIO is ready"
        tail -f /dev/null   
      '
    environment:
      MINIO_USER: ${MINIO_USER}
      MINIO_PASS: ${MINIO_PASS}
      S3_BUCKET:  ${S3_BUCKET}
    networks: [fraudnet]

  prediction:
    build: ./prediction
    restart: unless-stopped
    env_file:
      - .env

    depends_on:
      db:          
        condition: service_started
      minio:      
        condition: service_started
      rabbitmq:  
        condition: service_healthy
    networks: [fraudnet]
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  business:
    build: ./business
    env_file:
      - .env
      
    depends_on:
      db:          
        condition: service_started
      minio:       
        condition: service_started
      rabbitmq:    
        condition: service_healthy
    networks: [fraudnet]
    ports:
      - "8080:8000"

  # ─────────────────────────────────────────────────────────────────
  # Airflow (LocalExecutor) — web UI + scheduler + metadata database
  # ─────────────────────────────────────────────────────────────────

  airflow-db:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_db:/var/lib/postgresql/data
    networks: [fraudnet]

  airflow-init:
    image: apache/airflow:2.8.1-python3.11
    depends_on:
      airflow-db:
        condition: service_started
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      PYTHONPATH: "/opt/airflow/app"
    command: bash -c "airflow db upgrade && airflow users create --role Admin --username $$_AIRFLOW_WWW_USER_USERNAME --password $$_AIRFLOW_WWW_USER_PASSWORD --firstname Air --lastname Flow --email admin@example.org"
    networks: [fraudnet]
    volumes:
      - ./:/opt/airflow/app          # ➜ весь репозиторий
      - ./dags:/opt/airflow/dags

  airflow-webserver:
    image: apache/airflow:2.8.1-python3.11
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: "supersecretkey"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      _PIP_ADDITIONAL_REQUIREMENTS: "fastapi[all] torch torch_geometric catboost scikit-learn==1.6.1 scipy<=1.15.3 joblib pydantic pydantic-settings aioboto3 numpy sqlalchemy[asyncio] asyncpg python-dateutil category_encoders aio-pika pymon"
    ports:
      - "8089:8080"
    networks: [fraudnet]
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - ./:/opt/airflow/app

    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.8.1-python3.11
    depends_on:
      airflow-webserver:
        condition: service_started
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      PYTHONPATH: "/opt/airflow/app"
      _PIP_ADDITIONAL_REQUIREMENTS: "fastapi[all] torch torch_geometric catboost scikit-learn==1.6.1 scipy<=1.15.3 joblib pydantic pydantic-settings aioboto3 numpy==1.26.4 sqlalchemy[asyncio] asyncpg python-dateutil category_encoders aio-pika pymon"

    networks: [fraudnet]
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - ./:/opt/airflow/app          # ➜ весь репозиторий
      

      
    command: scheduler

volumes:
  pgdata:
  minio:
  airflow_db:
  airflow_logs:

networks:
  fraudnet: